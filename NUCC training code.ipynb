{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9eb8de18",
   "metadata": {},
   "source": [
    "# The following code snippet is a common set of import statements in Python for three popular libraries:\n",
    "\n",
    "## TensorFlow is an open-source machine learning framework developed by the Google Brain team. It provides tools for building and training various machine learning models.\n",
    "\n",
    "## Pandas is a powerful data manipulation and analysis library. It provides data structures like DataFrames, making it easy to work with structured data.\n",
    "\n",
    "## NumPy is a fundamental library for numerical computing in Python. It provides support for large, multi-dimensional arrays and matrices, along with a collection of high-level mathematical functions to operate on these arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e1b073",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6404e371",
   "metadata": {},
   "source": [
    "# The following code loads data from a CSV file into a Pandas DataFrame (train_df) and then prints the data types of each column in that DataFrame. The file path points to a CSV file containing the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c57c5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fp = './dataset/train.csv'\n",
    "train_df = pd.read_csv(train_fp);\n",
    "train_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91c8783",
   "metadata": {},
   "source": [
    "# The following function is a simple classifier for an assay result. If the assay string (after stripping and converting to lowercase) is 'positive', it returns 1; otherwise, it returns 0. This kind of function is commonly used when dealing with binary classification tasks where the goal is to map certain input conditions to binary outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bade50c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_fn(assay):\n",
    "    assay = assay.strip().lower()\n",
    "    if assay == 'positive':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e0c603",
   "metadata": {},
   "source": [
    "# x_train contains the input features for the machine learning model, excluding the columns 'Epitope', 'MHC', and 'Assay'. y_train contains the corresponding binary labels derived from the 'Assay' column using the target_fn function. This kind of data preparation is common in supervised learning, where the goal is to train a model to predict the target variable ('Assay' in this case) based on input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04135fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_df.drop(['Epitope', 'MHC','Assay'], axis=1)\n",
    "y_train = train_df.pop('Assay').apply(target_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edf8ab8",
   "metadata": {},
   "source": [
    "# The following will output the first five rows (by default) of the x_train DataFrame, allowing us to inspect the structure and content of the training features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3392d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521ae0ff",
   "metadata": {},
   "source": [
    "# The following will output the first five elements (by default) of the y_train Series, allowing us to inspect the binary labels associated with the corresponding rows in the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29358ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1e4413",
   "metadata": {},
   "source": [
    "# In the following code snippet, we are creating a new Series epitope_and_mhc_comb_train by concatenating the 'Epitope' and 'MHC' columns of the train_df DataFrame, and then converting the resulting strings to uppercase. Additionally, we are extracting the values of this Series into a NumPy array named epitope_and_mhc_train_texts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6e1eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "epitope_and_mhc_comb_train = (train_df['Epitope'] + train_df['MHC']).str.upper()\n",
    "epitope_and_mhc_train_texts = epitope_and_mhc_comb_train.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b840108",
   "metadata": {},
   "source": [
    "# In the following code snippet, we are importing various modules and classes from the TensorFlow Keras library, which is commonly used for building neural network models in machine learning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ab3a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input, Embedding, Activation, Flatten, Dense,concatenate,GRU\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Dropout,AveragePooling1D,BatchNormalization,Bidirectional\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad714dd9",
   "metadata": {},
   "source": [
    "# The following code configures a Tokenizer for character-level tokenization, fits it on the provided text data, and customizes its vocabulary using a predefined character dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a7652b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = Tokenizer(num_words=None, char_level=True, oov_token='UNK')\n",
    "tk.fit_on_texts(epitope_and_mhc_train_texts)\n",
    "alphabet = 'abcdefghijklmnopqrstuvwxyz0123456789-*:'\n",
    "char_dict = {}\n",
    "for i, char in enumerate(alphabet):\n",
    "    char_dict[char] = i + 1\n",
    "\n",
    "# Use char_dict to replace the tk.word_index\n",
    "tk.word_index = char_dict.copy()\n",
    "# Add 'UNK' to the vocabulary\n",
    "tk.word_index[tk.oov_token] = max(char_dict.values()) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9d1ac3",
   "metadata": {},
   "source": [
    "# The following code processes the text data (epitope_and_mhc_train_texts) by converting it to integer-encoded sequences using the configured Tokenizer and then pads the sequences to ensure a consistent length of 25. The resulting sequences are stored in train_sequences_pad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d569bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences = tk.texts_to_sequences(epitope_and_mhc_train_texts)\n",
    "train_sequences_pad = pad_sequences(train_sequences, maxlen=25)\n",
    "print(train_sequences[0])\n",
    "maxlen = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddce0a56",
   "metadata": {},
   "source": [
    "# The following code defines a complex neural network model using the Keras functional API. This model takes text data through a tokenizer and embedding layer, processes it with convolutional layers, and combines it with numerical features. It is designed for binary classification with a sigmoid output.We will explain the final model structure with legends and annotations in the article.\n",
    "\n",
    "## Tokenizer and Embedding Layer: vocab_size is set to the length of the vocabulary in the tokenizer. Embedding_size is set to 32, which represents the dimensionality of the embedding space.\n",
    "\n",
    "## Text Input and Embedding Layer: input_comb is the input layer for the text data, with a shape of maxlen.The Embedding layer converts integer-encoded sequences into dense vectors of fixed size (embedding_size).\n",
    "\n",
    "## Convolutional and Pooling Layers: Two sets of 1D convolutional layers followed by batch normalization and max-pooling. These layers are commonly used in text or sequence data processing for feature extraction.\n",
    "\n",
    "## Flatten and Dense Layers: Flatten layer to convert the output from the convolutional layers into a 1D array.Dense layers with ReLU activation, followed by dropout for regularization.\n",
    "\n",
    "## Numerical Input and Dense Layer: input_res is the input layer for numerical features with a shape of 3. A dense layer for processing numerical features followed by dropout.\n",
    "\n",
    "## Concatenation and Output Layers: Concatenation of the outputs from the text and numerical branches. Additional dense layers with ReLU activation, followed by the final output layer with a sigmoid activation for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053aba51",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tk.word_index)\n",
    "embedding_size = 32\n",
    "\n",
    "input_comb = Input(shape=maxlen, name='epitope_and_mhc')\n",
    "x = Embedding(vocab_size + 1, embedding_size, input_length=maxlen)(input_comb)\n",
    "x = Conv1D(16, 3, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling1D(pool_size=3)(x)\n",
    "x = Conv1D(32, 3, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling1D(pool_size=3)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Model(inputs=input_comb,outputs=x)\n",
    "input_res = Input(shape=3, name = 'numerical_features')\n",
    "y = Dense(128, activation='relu')(input_res)\n",
    "y = Dropout(0.2)(y)\n",
    "y = Model(inputs=input_res, outputs=y)\n",
    "combined_out = concatenate([x.output, y.output])\n",
    "z = Dense(128, activation='relu')(combined_out)\n",
    "z = Dense(1, activation='sigmoid')(z)\n",
    "complex_model = Model(inputs=[x.input, y.input], outputs=z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f33e78",
   "metadata": {},
   "source": [
    "# The following code compiles the complex_model using the Adam optimizer, binary cross-entropy loss, and accuracy as the evaluation metric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df7075a",
   "metadata": {},
   "outputs": [],
   "source": [
    "complex_model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b40e8cf",
   "metadata": {},
   "source": [
    "# The following code defines an EarlyStopping callback for use during the training of a neural network model.The EarlyStopping callback is designed to monitor the validation accuracy during training. If the validation accuracy does not improve by at least min_delta over the specified patience number of epochs, the training will be stopped early to avoid overfitting. \n",
    "\n",
    "## monitor='val_accuracy': This specifies the metric to monitor for early stopping. In this case, it's the validation accuracy (val_accuracy). The training process will stop when the specified metric stops improving.\n",
    "\n",
    "## min_delta=0.0001: This parameter sets the minimum change in the monitored metric to qualify as an improvement. If the change is less than this value, it won't be considered as an improvement.\n",
    "\n",
    "## patience=10: This is the number of epochs with no improvement after which training will be stopped. In this case, if there is no improvement in validation accuracy for 10 consecutive epochs, the training will stop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbf7255",
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystop_callback = EarlyStopping(\n",
    "  monitor='val_loss', min_delta=0.0001,\n",
    "  patience=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d806b6",
   "metadata": {},
   "source": [
    "# The following code is training the complex_model using the provided data and settings.The training history, including loss and accuracy metrics for both training and validation sets, will be stored in the history object.\n",
    "\n",
    "## x=[train_sequences_pad, x_train]: The input data for the model consists of two components â€“ the padded sequences (train_sequences_pad) and the numerical features (x_train).\n",
    "\n",
    "## y=y_train: The target data is the binary labels (y_train).\n",
    "\n",
    "## batch_size=64: The number of samples per gradient update. The model's weights are updated after processing each batch of 64 samples.\n",
    "\n",
    "## epochs=50: The number of times the entire training dataset is passed forward and backward through the neural network.\n",
    "\n",
    "## callbacks=[earlystop_callback]: The early stopping callback is applied during training. It will monitor the validation loss and stop training if there is no reduce for a certain number of epochs.\n",
    "\n",
    "## validation_split=0.2: Specifies that 20% of the training data will be used for validation. The model's performance on this validation set is monitored during training.\n",
    "\n",
    "## verbose=\"auto\": The verbosity mode during training. Setting it to \"auto\" means the verbosity level is set to 1 if a TQDM progress bar is used, and 2 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f484b1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = complex_model.fit([train_sequences_pad, x_train], y_train,\n",
    "                   batch_size=64,\n",
    "                   epochs=50,\n",
    "                   callbacks=[earlystop_callback],\n",
    "                   validation_split=0.2,\n",
    "                   verbose=\"auto\",\n",
    "                           )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
